## 数据倾斜
- Spark引擎的大部分task执行时间比较一致，但是存在一些task的执行时间特别长，例如，500个task，其中498个执行较快，10分钟执行完成，剩余的两个task需要执行半个小时以上。
- 例行化执行的代码，某一天发生OOM问题，大概率是有数据倾斜了。

数据倾斜产生的原因是：shuffle的时候，需要将各个节点的相同的key拉取到同一个节点上，如果这个key对应的数据量非常大的时候，就会发生数据倾斜。

数据倾斜只会发生在shuffle过程中，Spark引擎会触发Shuffle的RDD算子有：distinct、repartition、reduceByKey、groupByKey、aggregateByKey、join

## 常见解决方案

### 调整并行度
需要Shuffle的操作算子上直接设置并行度或者使用spark.default.parallelism设置。如果是Spark SQL，还可通过SET spark.sql.shuffle.partitions=num_tasks设置并行度。

该方法使用场景少，只能缓解数据倾斜，不能彻底解决数据倾斜。

### Map side join

通过Spark的Broadcast机制，将Reduce Join转化为Map Join，避免Shuffle，从而完全消除Shuffle带来的数据倾斜。

参与Join的一侧数据集足够小，并且主要适用于Join的场景，不适合聚合的场景，适用条件有限。

### 异常值过滤

通过Spark的reduceByKey，统计每一个key的数量，超过指定数量的key或者数量top的key，作为异常key。当然也可以使用Sample对RDD进行抽样后，进行key的统计。

该方法的特点是：简单、粗暴，有一定的适用场景。

### key值转换：加随机数
这个可以理解为大招

- 对于单个RDD的Shuffle操作，如groupByKey，将key值加上一个随机数的前缀。这样就需要执行二次聚合操作。
- 对于多个RDD的Shuffle操作，如join，将其中的一个有明显数据倾斜的RDD的key，加上n以内的随机数的前缀，另一个RDD的每一个key，都加上0-n的前缀，相当于RDD膨胀了n倍。

实际场景中可能需要上述方案的组合操作，比如：异常值过滤 + key值转换：加随机数，可以进行性能的优化：根据异常值，对RDD进行拆分：分别拆分成两个RDD，对于没有数据倾斜的，正常操作。对于有数据倾斜的加上随机前缀，再进行Shuffle操作。

个人经验：大部分数据倾斜问题，都是业务问题，了解你的数据分布、数据逻辑这个才是数据开发的王道。